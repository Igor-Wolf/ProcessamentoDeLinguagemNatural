{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "629285e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rei vs rainha: 0.60012286901474\n",
      "rei vs carro: 0.09075795114040375\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# EMBEDDING\n",
    "#--------------------------------------------------\n",
    "\n",
    "#python -m spacy download pt_core_news_md\n",
    "\n",
    "import spacy\n",
    "\n",
    "# Carregar modelo em português com embeddings\n",
    "nlp = spacy.load(\"pt_core_news_md\")\n",
    "\n",
    "# Criar embeddings\n",
    "palavra1 = nlp(\"rei\")\n",
    "palavra2 = nlp(\"rainha\")\n",
    "palavra3 = nlp(\"carro\")\n",
    "\n",
    "# Comparar similaridade\n",
    "print(\"rei vs rainha:\", palavra1.similarity(palavra2))\n",
    "print(\"rei vs carro:\", palavra1.similarity(palavra3))\n",
    "\n",
    "# basicamente busca a similaridade entre as palavras, o quanto uma se agrupa a outra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b8993c93",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\aaa\\venv\\lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in D:\\Huggingface\\hub\\models--t5-small. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 131/131 [00:00<00:00, 1180.18it/s, Materializing param=shared.weight]                                                      \n",
      "The model 'T5ForConditionalGeneration' is not supported for text-generation. Supported models are ['PeftModelForCausalLM', 'AfmoeForCausalLM', 'ApertusForCausalLM', 'ArceeForCausalLM', 'AriaTextForCausalLM', 'BambaForCausalLM', 'BartForCausalLM', 'BertLMHeadModel', 'BertGenerationDecoder', 'BigBirdForCausalLM', 'BigBirdPegasusForCausalLM', 'BioGptForCausalLM', 'BitNetForCausalLM', 'BlenderbotForCausalLM', 'BlenderbotSmallForCausalLM', 'BloomForCausalLM', 'BltForCausalLM', 'CamembertForCausalLM', 'LlamaForCausalLM', 'CodeGenForCausalLM', 'CohereForCausalLM', 'Cohere2ForCausalLM', 'CpmAntForCausalLM', 'CTRLLMHeadModel', 'CwmForCausalLM', 'Data2VecTextForCausalLM', 'DbrxForCausalLM', 'DeepseekV2ForCausalLM', 'DeepseekV3ForCausalLM', 'DiffLlamaForCausalLM', 'DogeForCausalLM', 'Dots1ForCausalLM', 'ElectraForCausalLM', 'Emu3ForCausalLM', 'ErnieForCausalLM', 'Ernie4_5ForCausalLM', 'Ernie4_5_MoeForCausalLM', 'Exaone4ForCausalLM', 'FalconForCausalLM', 'FalconH1ForCausalLM', 'FalconMambaForCausalLM', 'FlexOlmoForCausalLM', 'FuyuForCausalLM', 'GemmaForCausalLM', 'Gemma2ForCausalLM', 'Gemma3ForConditionalGeneration', 'Gemma3ForCausalLM', 'Gemma3nForConditionalGeneration', 'Gemma3nForCausalLM', 'GitForCausalLM', 'GlmForCausalLM', 'Glm4ForCausalLM', 'Glm4MoeForCausalLM', 'Glm4MoeLiteForCausalLM', 'GotOcr2ForConditionalGeneration', 'GPT2LMHeadModel', 'GPT2LMHeadModel', 'GPTBigCodeForCausalLM', 'GPTNeoForCausalLM', 'GPTNeoXForCausalLM', 'GPTNeoXJapaneseForCausalLM', 'GptOssForCausalLM', 'GPTJForCausalLM', 'GraniteForCausalLM', 'GraniteMoeForCausalLM', 'GraniteMoeHybridForCausalLM', 'GraniteMoeSharedForCausalLM', 'HeliumForCausalLM', 'HunYuanDenseV1ForCausalLM', 'HunYuanMoEV1ForCausalLM', 'Jais2ForCausalLM', 'JambaForCausalLM', 'JetMoeForCausalLM', 'Lfm2ForCausalLM', 'Lfm2MoeForCausalLM', 'LlamaForCausalLM', 'Llama4ForCausalLM', 'Llama4ForCausalLM', 'LongcatFlashForCausalLM', 'MambaForCausalLM', 'Mamba2ForCausalLM', 'MarianForCausalLM', 'MBartForCausalLM', 'MegatronBertForCausalLM', 'MiniMaxForCausalLM', 'MiniMaxM2ForCausalLM', 'MinistralForCausalLM', 'Ministral3ForCausalLM', 'MistralForCausalLM', 'MixtralForCausalLM', 'MllamaForCausalLM', 'ModernBertDecoderForCausalLM', 'MoshiForCausalLM', 'MptForCausalLM', 'MusicgenForCausalLM', 'MusicgenMelodyForCausalLM', 'MvpForCausalLM', 'NanoChatForCausalLM', 'NemotronForCausalLM', 'OlmoForCausalLM', 'Olmo2ForCausalLM', 'Olmo3ForCausalLM', 'OlmoeForCausalLM', 'OpenAIGPTLMHeadModel', 'OPTForCausalLM', 'PegasusForCausalLM', 'PersimmonForCausalLM', 'PhiForCausalLM', 'Phi3ForCausalLM', 'Phi4MultimodalForCausalLM', 'PhimoeForCausalLM', 'PLBartForCausalLM', 'ProphetNetForCausalLM', 'Qwen2ForCausalLM', 'Qwen2MoeForCausalLM', 'Qwen3ForCausalLM', 'Qwen3MoeForCausalLM', 'Qwen3NextForCausalLM', 'RecurrentGemmaForCausalLM', 'ReformerModelWithLMHead', 'RemBertForCausalLM', 'RobertaForCausalLM', 'RobertaPreLayerNormForCausalLM', 'RoCBertForCausalLM', 'RoFormerForCausalLM', 'RwkvForCausalLM', 'SeedOssForCausalLM', 'SmolLM3ForCausalLM', 'SolarOpenForCausalLM', 'StableLmForCausalLM', 'Starcoder2ForCausalLM', 'TrOCRForCausalLM', 'VaultGemmaForCausalLM', 'WhisperForCausalLM', 'XGLMForCausalLM', 'XLMWithLMHeadModel', 'XLMRobertaForCausalLM', 'XLMRobertaXLForCausalLM', 'XLNetLMHeadModel', 'xLSTMForCausalLM', 'XmodForCausalLM', 'ZambaForCausalLM', 'Zamba2ForCausalLM'].\n",
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n",
      "Passing `generation_config` together with generation-related arguments=({'max_length', 'do_sample', 'num_return_sequences'}) is deprecated and will be removed in future versions. Please pass either a `generation_config` object OR all generation parameters explicitly, but not both.\n",
      "Both `max_new_tokens` (=256) and `max_length`(=120) seem to have been set. `max_new_tokens` will take precedence. Please refer to the documentation for more information. (https://huggingface.co/docs/transformers/main/en/main_classes/text_generation)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SUMMARY: summarize:  Artificial intelligence is transforming the world. With the help of transformer models,  computers can now perform complex tasks like translation, summarization, and automated  responses with high accuracy. Recent advances allow machines to understand complex contexts  and generate coherent text. This technology is being applied in areas like customer service,  healthcare, and education. According to research, the AI market will grow 30% annually  over the next decade, creating new opportunities. \n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# TRANSFORMERS (VERSÃO ANTIGA)\n",
    "#--------------------------------------------------\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# Modelo clássico de resumo (compatível com versões antigas)\n",
    "summarizer = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"t5-small\"\n",
    ")\n",
    "\n",
    "text = \"\"\"\n",
    "Artificial intelligence is transforming the world. With the help of transformer models, \n",
    "computers can now perform complex tasks like translation, summarization, and automated \n",
    "responses with high accuracy. Recent advances allow machines to understand complex contexts \n",
    "and generate coherent text. This technology is being applied in areas like customer service, \n",
    "healthcare, and education. According to research, the AI market will grow 30% annually \n",
    "over the next decade, creating new opportunities.\n",
    "\"\"\"\n",
    "\n",
    "# IMPORTANTE: prefixo \"summarize:\"\n",
    "prompt = \"summarize: \" + text.replace(\"\\n\", \" \")\n",
    "\n",
    "summary = summarizer(\n",
    "    prompt,\n",
    "    max_length=120,\n",
    "    do_sample=False,\n",
    "    num_return_sequences=1\n",
    ")\n",
    "\n",
    "print(\"SUMMARY:\", summary[0][\"generated_text\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ba2e600",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\aaa\\venv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading weights: 100%|██████████| 201/201 [00:00<00:00, 967.65it/s, Materializing param=classifier.weight]                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Frase: Este curso é excelente! \n",
      "Sentimento: 5 stars com confiança de 0.70\n",
      "\n",
      "Frase: Não gostei do conteúdo. \n",
      "Sentimento: 1 star com confiança de 0.42\n",
      "\n",
      "Frase: Achei a explicação clara e objetiva. \n",
      "Sentimento: 4 stars com confiança de 0.47\n",
      "\n",
      "Frase: O conteúdo é bom, mas não foi objetivo \n",
      "Sentimento: 3 stars com confiança de 0.44\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------------------------------\n",
    "# FINE TUNING\n",
    "#--------------------------------------------------\n",
    "from transformers import pipeline\n",
    "\n",
    "classificador = pipeline(\"sentiment-analysis\", model=\"nlptown/bert-base-multilingual-uncased-sentiment\")\n",
    "\n",
    "frases = [\n",
    "    \"Este curso é excelente!\",\n",
    "    \"Não gostei do conteúdo.\",\n",
    "    \"Achei a explicação clara e objetiva.\",\n",
    "    \"O conteúdo é bom, mas não foi objetivo\"\n",
    "]\n",
    "\n",
    "for frase in frases:\n",
    "    resultado = classificador(frase)[0]\n",
    "    print(f\"Frase: {frase} \\nSentimento: {resultado['label']} com confiança de {resultado['score']:.2f}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
